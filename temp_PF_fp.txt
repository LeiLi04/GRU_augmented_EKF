"""FilterPy-inspired Particle Filter demo on Lorenz dataset."""
from __future__ import annotations

import argparse
from collections import namedtuple
from pathlib import Path
import pickle

import numpy as np
from filterpy.monte_carlo import systematic_resample


def dB_to_lin(x_dB: float) -> float:
    """Convert decibel value to linear scale."""
    return 10.0 ** (x_dB / 10.0)


def lin_to_dB(x_lin: float, eps: float = 1e-12) -> float:
    """Convert linear value to decibels with a numerical floor."""
    return 10.0 * np.log10(max(x_lin, eps))


def lorenz_rhs(x: np.ndarray, sigma: float = 10.0, rho: float = 28.0, beta: float = 8.0 / 3.0) -> np.ndarray:
    x1, x2, x3 = x
    return np.array(
        [
            sigma * (x2 - x1),
            x1 * (rho - x3) - x2,
            x1 * x2 - beta * x3,
        ],
        dtype=float,
    )


def lorenz_rk4(x: np.ndarray, dt: float) -> np.ndarray:
    k1 = lorenz_rhs(x)
    k2 = lorenz_rhs(x + 0.5 * dt * k1)
    k3 = lorenz_rhs(x + 0.5 * dt * k2)
    k4 = lorenz_rhs(x + dt * k3)
    return x + (dt / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)


def _load_lorenz_dataset(path: Path):
    class _CompatUnpickler(pickle.Unpickler):
        def find_class(self, module, name):
            if module.startswith("numpy._core"):
                module = module.replace("numpy._core", "numpy.core")
            return super().find_class(module, name)

    with path.open("rb") as handle:
        return _CompatUnpickler(handle).load()


def _setup_matplotlib():
    import matplotlib

    matplotlib.use("Agg")

    import matplotlib.cbook as cbook

    if not hasattr(cbook, "_is_pandas_dataframe"):
        def _is_pandas_dataframe(_obj):
            return False

        cbook._is_pandas_dataframe = _is_pandas_dataframe

    if not hasattr(cbook, "_Stack") and hasattr(cbook, "Stack"):
        cbook._Stack = cbook.Stack

    if not hasattr(cbook, "_ExceptionInfo"):
        cbook._ExceptionInfo = namedtuple("_ExceptionInfo", "exc_class exc traceback")

    if not hasattr(cbook, "_broadcast_with_masks"):
        def _broadcast_with_masks(*arrays):
            return tuple(np.broadcast_arrays(*[np.asarray(arr) for arr in arrays]))

        cbook._broadcast_with_masks = _broadcast_with_masks

    import matplotlib.pyplot as plt  # noqa: F401


def predict_particles(particles: np.ndarray, dt: float, q_std: float) -> np.ndarray:
    """Propagate particles with Lorenz dynamics plus Gaussian process noise."""
    for i in range(particles.shape[0]):
        particles[i] = lorenz_rk4(particles[i], dt)
    particles += q_std * np.random.randn(*particles.shape)
    return particles


def measurement_likelihood(z: np.ndarray, particles: np.ndarray, r_std: float) -> np.ndarray:
    """Compute unnormalized likelihood p(z | x) assuming Gaussian noise diag(r_std^2)."""
    diffs = particles - z  # broadcasting: (N, state_dim)
    inv_var = 1.0 / (r_std**2)
    # ignoring normalization constant since weights will be renormalized
    weights = np.exp(-0.5 * np.sum(diffs * diffs * inv_var, axis=1))
    weights += 1e-300  # avoid zeros
    return weights


def normalize_weights(weights: np.ndarray) -> np.ndarray:
    total = np.sum(weights)
    if total <= 0.0:
        return np.ones_like(weights) / weights.size
    return weights / total


def effective_sample_size(weights: np.ndarray) -> float:
    return 1.0 / np.sum(np.square(weights))


def resample_from_index(particles: np.ndarray, weights: np.ndarray, indices: np.ndarray) -> np.ndarray:
    resampled = particles[indices]
    weights.fill(1.0 / weights.size)
    return resampled


def main():
    parser = argparse.ArgumentParser(description="Bootstrap Particle Filter demo on Lorenz dataset (FilterPy-style).")
    parser.add_argument(
        "--dataset",
        default="src/data/trajectories/trajectories_m_3_n_3_LorenzSSM_data_T_1000_N_500_r2_20.0dB_nu_-10.0dB.pkl",
        help="Path to Lorenz trajectory pickle.",
    )
    parser.add_argument("--index", type=int, default=0, help="Sample index to visualise (default: 0).")
    parser.add_argument("--delta_t", type=float, default=0.02, help="Integration time step for dynamics.")
    parser.add_argument("--inverse_r2_dB", type=float, default=20.0, help="Inverse measurement noise power in dB.")
    parser.add_argument("--nu_dB", type=float, default=-10.0, help="Process vs measurement noise ratio in dB.")
    parser.add_argument("--num_particles", type=int, default=500, help="Number of particles.")
    parser.add_argument(
        "--resample_threshold",
        type=float,
        default=0.5,
        help="Trigger resampling when N_eff / N < threshold (default 0.5).",
    )
    parser.add_argument("--seed", type=int, default=123, help="Random seed for reproducibility.")
    args = parser.parse_args()

    np.random.seed(args.seed)

    dataset_path = Path(args.dataset)
    if not dataset_path.exists():
        raise FileNotFoundError(f"Dataset not found: {dataset_path}")

    payload = _load_lorenz_dataset(dataset_path)
    num_samples = payload["num_samples"]
    if not (0 <= args.index < num_samples):
        raise IndexError(f"index {args.index} out of range (0 ā‰?idx < {num_samples}).")

    sample_X, sample_Y = payload["data"][args.index]
    sample_X = np.asarray(sample_X, dtype=float)
    sample_Y = np.asarray(sample_Y, dtype=float)

    dt = float(args.delta_t)
    n_states = sample_X.shape[1]

    r2 = 1.0 / dB_to_lin(args.inverse_r2_dB)
    q2 = dB_to_lin(args.nu_dB - args.inverse_r2_dB)
    r_std = np.sqrt(r2)
    q_std = np.sqrt(q2)

    # Initialize particles around the first observation with broad covariance.
    particles = sample_Y[0] + np.random.randn(args.num_particles, n_states) * 2.0
    weights = np.ones(args.num_particles, dtype=float) / args.num_particles

    estimates = np.zeros_like(sample_Y)

    for t in range(sample_Y.shape[0]):
        # Predict
        particles = predict_particles(particles, dt=dt, q_std=q_std)

        # Update weights
        weights *= measurement_likelihood(sample_Y[t], particles, r_std=r_std)
        weights = normalize_weights(weights)

        # Compute estimate
        estimates[t] = np.average(particles, weights=weights, axis=0)

        # Resample if needed
        n_eff = effective_sample_size(weights)
        if n_eff / args.num_particles < args.resample_threshold:
            indices = systematic_resample(weights)
            particles = resample_from_index(particles, weights, indices)

    x_hat = np.zeros_like(sample_X)
    x_hat[0] = sample_X[0]
    x_hat[1:] = estimates

    mse_per_step = np.mean((x_hat[1:] - sample_X[1:]) ** 2, axis=1)
    mse_lin = float(np.mean(mse_per_step))
    mse_dB = lin_to_dB(mse_lin)

    time_axis = np.arange(sample_Y.shape[0])

    _setup_matplotlib()
    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D  # noqa: F401

    # 3D trajectories
    fig = plt.figure(figsize=(18, 5))
    ax_true = fig.add_subplot(1, 3, 1, projection="3d")
    ax_obs = fig.add_subplot(1, 3, 2, projection="3d")
    ax_est = fig.add_subplot(1, 3, 3, projection="3d")

    ax_true.plot(
        sample_X[1:, 0],
        sample_X[1:, 1],
        sample_X[1:, 2],
        label="x (true)",
        color="tab:blue",
        linewidth=1.5,
    )
    ax_true.set_title("True state x")
    ax_true.set_xlabel("x1")
    ax_true.set_ylabel("x2")
    ax_true.set_zlabel("x3")
    ax_true.legend(fontsize="small")

    ax_obs.plot(
        sample_Y[:, 0],
        sample_Y[:, 1],
        sample_Y[:, 2],
        label="y (obs)",
        color="tab:green",
        linewidth=1.2,
    )
    ax_obs.set_title("Observation y")
    ax_obs.set_xlabel("y1")
    ax_obs.set_ylabel("y2")
    ax_obs.set_zlabel("y3")
    ax_obs.legend(fontsize="small")

    ax_est.plot(
        x_hat[1:, 0],
        x_hat[1:, 1],
        x_hat[1:, 2],
        label="$\\hat{x}$ (estimate)",
        color="tab:orange",
        linewidth=1.5,
    )
    ax_est.set_title("Estimate $\\hat{x}$")
    ax_est.set_xlabel("x1")
    ax_est.set_ylabel("x2")
    ax_est.set_zlabel("x3")
    ax_est.legend(fontsize="small")

    fig.tight_layout()
    fig.savefig("lorenz_pf_fp_states.png", dpi=300)

    # Per-step MSE plot
    fig_mse = plt.figure(figsize=(10, 3))
    plt.plot(time_axis, mse_per_step, label="per-step MSE", color="tab:red")
    plt.axhline(0.0, color="black", linewidth=0.8, linestyle=":", label="zero baseline")
    plt.ylabel("MSE")
    plt.xlabel("Time step")
    plt.title("Particle Filter per-step state estimation MSE")
    plt.legend()
    plt.grid(True, linewidth=0.3, alpha=0.7)
    fig_mse.tight_layout()
    fig_mse.savefig("lorenz_pf_fp_mse.png", dpi=300)

    print(f"Summary stats -> MSE_lin: {mse_lin:.3f}, MSE_dB: {mse_dB:.3f}")

    plt.show()


if __name__ == "__main__":
    main()
